---
title: Data analysis on campus recruitment
author: Xuxin Zhang
date: '2020-09-14'
slug: data-analysis-on-campus-recruitment
categories:
  - R project
tags:
  - tidymodel
  - machine learning
  - data visualization
subtitle: ''
summary: ''
authors: []
lastmod: '2020-09-14T12:54:08+08:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
## 1. Introduction
Today, we will analyze the data on campus recruitment. The data is simulated for MBA students at XYZ campus and includes the students' performance from middle school all the way to MBA. It also contains the application status of a student as well as his/her salary if placed. In this project, we aim to unfold the key factors that determines the application status. In order to achieve this goal, we first do some primary data analysis and then build a machine learning model which hopefully could predict whether a student would be placed or not. You can access the data in the following websites.

https://www.kaggle.com/benroshan/factors-affecting-campus-placement

## 2. Preliminary data analysis

First, we need to import our data and loading the packages we are going to use.
```{r,message=FALSE,warning=FALSE}
library(tidyverse)
library(tidymodels)
library(plotly)
campus <- read_csv("/Users/xuxian/Documents/UCLA related/R/Projects/tidymodel/campus recruit/campus recruit.csv")
theme_set(theme_light())
```

We can take a look at the data we have. Notice that the names of the columns could be really confusing, so we need to change them into clearer names.
```{r}
campus
```

```{r}
campus<-campus%>%transmute(sl_no,gender,
                   middle_school_score = ssc_p,
                   middle_school_board = ssc_b,
                   high_school_score = hsc_p, 
                   high_school_board = hsc_b, 
                   high_school_special = hsc_s, 
                   college_score = degree_p, 
                   college_major = degree_t, 
                   workex, employ_test = etest_p,
                   mba_special = specialisation,
                   mba_score = mba_p, status,
                   salary)
campus
```


Since our goal in this project is to create a model to predict whether a peron is placed or not, we can first take a look at how many people are placed in this data set.

```{r}
campus%>%count(status)
```

To study which factors can influence the placing status, we first start from analyzing how they perform in their middle school. 
```{r}
campus%>%ggplot(aes(x = middle_school_score, fill = status))+
    geom_histogram(aes(y = ..density..), alpha = 0.5, color = "white")+
    geom_density(alpha = 0.6)+scale_fill_brewer(palette = "Paired")+
    labs(x = "Scores at middle school", y = "Density", title = "Distribution of scores when the applicants were in middle school")
```
In this visualization, we see that generally the applicants who receive the offer (placed) have a higher score in middle school. 

Next, we turn to the type of board of education for these applicants.

```{r}
library(ggplot2)
campus%>%
    ggplot(aes(x = middle_school_score, fill = status))+
    geom_histogram(aes(y = ..density..), alpha = 0.5, color = "white")+
    geom_density(alpha = 0.6)+
    scale_fill_brewer(palette = "Paired")+facet_wrap(~middle_school_board,ncol = 1)+
    labs(x = "Scores at middle school", y = "Density", title = "Distribution of scores when the applicants were in middle school")
```

Based on this graph, we could see that the scores for the people with central board of education diverges less compared to the those of people with other boards. This pattern is actually a reflection of the distribution of educational resources among middle schools: the reason why the scores of applicants studying in central board of education don't differ that much is becauses they receive almost equal amount of educational resources. The difference is mostly caused by the the individual difference among the applicants; however, for schools with other types of board of education, in addition to the individual difference among the students, educational resources also play an important role in determining the scores of the students. For example, some schools may have  a better team of teachers than the others. Gradually, these difference of educational resources would inevitably lead to the diverge of students performance. 

Then we would proceed to their high schools. Again, we analyze the distribution of their scores for placed and unplaced applicants. From the following graph, we see the high school scores of unplaced applicants are generally lower than those of placed applicants, which means the high schools score is another important factor.

```{r}
campus%>%
  ggplot(aes(x = high_school_score, fill = status))+
  geom_histogram(aes(y = ..density..), alpha = 0.5, color = "white")+
  geom_density(alpha = 0.6)+scale_fill_brewer(palette = "Paired")+
  labs(x = "Scores at high school", y = "Density", title = "Distribution of scores when the applicants were in high school")
```

We can exploit more information by adding another variable to our plot above: `high school specialization`. From this graph, we can see two unusual spikes in the density for placed applicants specialized in Arts. My guess for this phenomenon is that Arts specialization is not bonus point in the recruitment process, so the applicants specialized in Arts during their high schools must be really excellent so that they could be considered by the recruiters.
```{r}
campus%>%
  ggplot(aes(x = high_school_score, fill = status))+
  geom_histogram(aes(y = ..density..), alpha = 0.5, color = "white")+
  geom_density(alpha = 0.6)+
  scale_fill_brewer(palette = "Paired")+facet_wrap(~high_school_special)+
  labs(x = "Scores at high school", y = "Density", title = "Distribution of scores for applicants with different high school specialization")
```

After finishing the analysis of the applicants' performance in their high school, we then turn to their performance in college. As we can see, there still exists a diverge between placed applicants' scores and unplaced applicants' scores, which means the scores at college is also an important factor to consider during the recruiting process. 

```{r}
campus%>%
  ggplot(aes(x = college_score, fill = status))+
  geom_histogram(aes(y = ..density..), alpha = 0.5, color = "white")+
  geom_density(alpha = 0.6)+scale_fill_brewer(palette = "Paired")+
  labs(x = "Scores at college", y = "Density", title = "Distribution of scores when the applicants were in college")
```

Here, we see that for students studying other majors, there is no significant difference between the scores of placed and unplaced students. On the contrarory, we see that the diveregnce occurs in both Comm&Mgmt and Sci&Tech majors. 
```{r}
campus%>%
  ggplot(aes(x = college_score, fill = status))+
  geom_histogram(aes(y = ..density..), alpha = 0.5, color = "white")+
  geom_density(alpha = 0.6)+
  scale_fill_brewer(palette = "Paired")+facet_wrap(~college_major)+
  labs(x = "Scores at college", y = "Density", title = "Distribution of scores for college students with different majors")
```

Finally, we will analyze their MBA performance. Quite surprisingly, there are only two options for their MBA specialization: Mk&Fin and Mk&HR. Furthermore, we see that among those who specialized in Mk&Fin, there is a larger proportion of people who get placed than those specialized in Mk&HR, which means the company has a greater demand for people specialized in Mk&Fin. 

```{r}
ggplotly(campus%>%
  count(mba_special,status)%>%
  ggplot(aes(x = mba_special, y = n, fill = status))+
  geom_col(position = "fill")+scale_fill_brewer(palette = "Paired")+
  labs(x = "MBA specialization", y = "Proportion", title = "The proportion of applicants being placed in two MBA specialization"))
```

Based on the following graph, we see that in the more demanded specialization area of Mk&Fin, the average MBA scores of placed applicants are higher. However, for the the other specialization, the placed applicants don't show any advantage in their scores. That is to say, the MBA score is a determining factor for recruitment only if the applicant is specialized in Mk&Fin. If the applicant specializes in Mk&HR, then the scores may not be as importnant. 

```{r}
campus%>%
  ggplot(aes(x = mba_special, y = mba_score, fill = status))+
  geom_boxplot(alpha = 0.7)+scale_fill_brewer(palette = "Paired")+
  labs(x = "MBA specialization", y = "MBA scores", title = "The score distribution in two MBA specialization")
```

We can take a step further to see if the MBA scores would affect the salary for the placed applicants.

Depends on the graph below, we see that for people who have previous working experience, between the MBA scores of 55 and 67, they tend to have a higher salary than those who have no working experience. The situation changes when the scores for applicants are greater than 67. This naturally leads us to ask the next question: does people with previous working experience generally have higher salary level?

```{r}
ggplotly(campus%>%
           ggplot(aes(x = mba_score, y = salary, color = workex))+
           geom_point(alpha = 0.6)+
           geom_smooth(se = FALSE)+
           scale_color_brewer(palette = "Paired")+scale_y_log10()+
           labs(x = "MBA score", y = "Salary", title = "The relationship between MBA scores and salary"))
```

The answer is revealed in the following graph. We see that the average salary is pretty much the same for both groups. The only difference is that there are more outlier points for the group with working experience. That is to say, generally, salary doesn't depend on previous working experience. However, if you do have working experience, it is more likely for you to get unusually high salary.

```{r}
ggplotly(campus%>%
  ggplot(aes(x = workex, y = salary, fill = workex))+geom_boxplot(alpha = 0.7)+
  scale_y_log10()+
  scale_fill_brewer(palette = "Paired")+
  labs(x = "Working experience", y = "Salary", title = "The salary distribution in regard of working experience"))
```

After all these analysis, we are finally ready to build our model.

## 3. Building the machine learning model
First we will clean our data once more.
```{r}
campus_df<-campus%>%select(-sl_no)%>%mutate_if(is.character, factor)
```

Next, we will split our data into training and testing data. In order to compare the performance of different models later, we ill create a bootstraps object based on our training data.

```{r}
set.seed(123)
campus_split <- initial_split(campus_df)

campus_train <- training(campus_split)
campus_test <- testing(campus_split)

campus_boot <- bootstraps(campus_train)
```

After getting the training data, we will pre-process our data with the help of `recipe` function in the `tidymodels` package. 

```{r}
campus_recipe<-recipe(status~., data = campus_train)%>%
  step_dummy(all_nominal(), -all_outcomes())%>%
  step_knnimpute(salary)%>%
  step_normalize(all_predictors())%>%
  step_corr(all_predictors())%>%
  step_zv(all_predictors())

prep(campus_recipe)%>%juice()

testing_proc <- bake(prep(campus_recipe), new_data = campus_test)
```

Now we can start to set up the models. In this project, we will build 4 models: a logistic regression model, a random forrest model, a nearest neighbor model, and a decision tree model. 
```{r}
lr_model <- logistic_reg()%>%set_engine("glm")

rf_model <- rand_forest()%>%set_mode("classification")%>%set_engine("ranger")

nn_model <- nearest_neighbor()%>%set_mode("classification")%>%set_engine("kknn")

tree_model <- decision_tree()%>%set_mode("classification")%>%set_engine("rpart")

```

Then we will create workflow based on the recipe and models we created earlier.

```{r}
lr_wf<-workflow()%>%add_recipe(campus_recipe)%>%add_model(lr_model)

rf_wf<-workflow()%>%add_recipe(campus_recipe)%>%add_model(rf_model)

nn_wf<-workflow()%>%add_recipe(campus_recipe)%>%add_model(nn_model)

tree_wf<-workflow()%>%add_recipe(campus_recipe)%>%add_model(tree_model)

```

Finally, we are ready to fit our models with the bootstraps object we created.

```{r,message=FALSE}
lr_result<-lr_wf%>%fit_resamples(resamples = campus_boot, control = control_resamples(save_pred = TRUE,verbose = TRUE))

rf_result<-rf_wf%>%fit_resamples(resamples = campus_boot, control = control_resamples(save_pred = TRUE,verbose = TRUE))

nn_result<-nn_wf%>%fit_resamples(resamples = campus_boot, control = control_resamples(save_pred = TRUE,verbose = TRUE))

tree_result<-tree_wf%>%fit_resamples(resamples = campus_boot, control = control_resamples(save_pred = TRUE,verbose = TRUE))
```

By collecting the metrics, we can evaluate our models. Based on the two metrics we have, we can see that the random forrest model performs the best both in accuracy and roc_auc among the four models.

```{r}
lr_result%>%collect_metrics()
rf_result%>%collect_metrics()
nn_result%>%collect_metrics()
tree_result%>%collect_metrics()
```

We can actually visualize the result using roc curve:

```{r}
result_total<-lr_result%>%collect_predictions()%>%mutate(model = "lr")%>%bind_rows(rf_result%>%collect_predictions()%>%mutate(model = "rf"))%>%bind_rows(nn_result%>%collect_predictions()%>%mutate(model = "nn"))%>%bind_rows(tree_result%>%collect_predictions()%>%mutate(model = "tree"))

result_total%>%group_by(model)%>%roc_curve(status,`.pred_Not Placed`)%>%autoplot()
```

As a result, we are going to use random forrest model for the remaining part of this object. Once we have determined the model, we can fit one last time on our testing data. We can see this time, we have an estimate of 0.961, which is highly accurate in predicting the applicantion status. 

```{r}
set.seed(1234)
rf_model%>%set_engine("ranger", importance = "permutation")%>%
  fit(status~., prep(campus_recipe)%>%juice)%>%
  predict( new_data = testing_proc,type = "prob")%>%
  mutate(truth = testing_proc$status)%>%
  roc_auc(truth, `.pred_Not Placed`)
```

We can take a look at important factors in our random forrest model. From the graph, we can see the top four determining factors are `middel_school_score`, `college_score`, `high_school_score`, and `mba_score`, so academic performance is in fact really important. The reason why `middle_school_score` has the highest importance may be due to the fact the scores of placed and unplaced applicants already have diverged quite a lot in middle school. We notice that `gender_M` also has a comparablly high importance, which means gender discrimination still exists during the recruitment process. 

```{r}
set.seed(2345)
library(vip)
rf_vip<-rf_model%>%
  set_engine("ranger", importance = "permutation")%>%
  fit(status~., prep(campus_recipe)%>%juice)%>%
  vi()

rf_vip%>%ggplot(aes(x = Importance,
                    y = reorder(Variable, Importance)))+
  geom_point()+
  geom_segment(aes(x = 0, 
                   xend = rf_vip$Importance, 
                   y =  reorder(Variable, Importance), 
                   yend =  reorder(Variable, Importance)))+
  labs(y = "Variables", title = "The Variable of Importance for the random forrest model")
```



