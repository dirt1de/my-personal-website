blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
library(tidyverse)
library(tidytext)
library(schrute)
library(ggridges)
library(plotly)
library(tidymodels)
theme_set(theme_light())
office_script<-schrute::theoffice
ratings_raw <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-17/office_ratings.csv")
View(office_script)
ratings_raw%>%mutate(total_ep = row_number())%>%
ggplot(aes(x = total_ep, y = imdb_rating))+
geom_point(aes(color = as.factor(season)))+
geom_path(aes(color = as.factor(season)))+
geom_text(aes(label= title),hjust = 1,check_overlap = TRUE)+
theme(legend.position = "null")+expand_limits(x = -20)+
labs(x = "Episode number", y = "IMDb rating", title = "How the rating of each episode changes over time")+geom_smooth()
ggplotly(ratings_raw%>%mutate(total_ep = row_number())%>%
ggplot(aes(x = total_ep, y = imdb_rating))+
geom_point(aes(color = as.factor(season)))+
geom_path(aes(color = as.factor(season)))+
geom_text(aes(label= title),hjust = 1,check_overlap = TRUE)+
theme(legend.position = "null")+expand_limits(x = -20)+
labs(x = "Episode number", y = "IMDb rating", title = "How the rating of each episode changes over time")+geom_smooth())
ratings_raw%>%mutate(total_ep = row_number())%>%
ggplot(aes(x = total_ep, y = imdb_rating))+
geom_point(aes(color = as.factor(season)))+
geom_path(aes(color = as.factor(season)))+
geom_text(aes(label= title),hjust = 1,check_overlap = TRUE)+
theme(legend.position = "null")+expand_limits(x = -20)+
labs(x = "Episode number", y = "IMDb rating", title = "How the rating of each episode changes over time")+geom_smooth()
ratings_raw%>%
ggplot(aes(y = as.factor(season),
x = imdb_rating,
fill = as.factor(season),
group = as.factor(season)))+
geom_density_ridges(alpha = 0.5, show.legend = FALSE)+
expand_limits(y = 10.7)+
labs(x = "IMDb rating", y = "Season number", title = "IMDb rating distribution for each season")
library(DT)
office_script%>%kable()
office_script%>%table()
??kable
office_script%>%datatable()
office_script%>%datatable()
office_script
office_script%>%head()
office_script%>%head(10)
ratings_raw %>%head(10)
episode_sent<-office_script%>%
select(season,episode, episode_name,character,text)%>%
unnest_tokens(word,text)%>%anti_join(stop_words)%>%
inner_join(get_sentiments("afinn"))%>%group_by(season,episode, episode_name)%>%
summarise(sentiment = sum(value))%>%ungroup()%>%
transmute(total_ep = row_number(),episode_name, sentiment,season)
episode_sent%>%
ggplot(aes(x = total_ep, y = sentiment))+
geom_point(aes(color = as.factor(season)))+
geom_path(aes(color = as.factor(season)))+
geom_text(aes(label= episode_name),hjust = 1,check_overlap = TRUE)+
theme(legend.position = "null")+expand_limits(x = -20)+
labs(x = "Episode number", y = "Sentiment level", title = "How the sentiment level of each epsiode changes over time")+geom_smooth()
episode_sent%>%
ggplot(aes(y = as.factor(season),
x = sentiment,
fill = as.factor(season),
group = as.factor(season)))+
geom_density_ridges(alpha = 0.5, show.legend = FALSE)+
expand_limits(y = 10.7)+
labs(x = "Sentiment level", y = "Season number", title = "Sentiment level distribution for each season")
library(ggrepel)
ratings_raw%>%mutate(total_ep = row_number())%>%
left_join(episode_sent)%>%
ggplot(aes(x = sentiment, y = imdb_rating ))+
geom_point(aes(color = as.factor(season)))+
geom_smooth()+labs(x = "Sentiment level", y = "IMDb rating", title = "The relationship between the sentiment level and the IMDb rating of an episode", color = "Season")
ratings_raw%>%mutate(total_ep = row_number())%>%
left_join(episode_sent)%>%
ggplot(aes(x = sentiment, y = imdb_rating ))+
geom_point(aes(color = as.factor(season)))+
geom_smooth()+labs(x = "Sentiment level", y = "IMDb rating", title = "The relationship between the sentiment level and the IMDb rating of an episode", color = "Season")+scale_y_log10()
ratings_raw%>%mutate(total_ep = row_number())%>%
left_join(episode_sent)%>%
ggplot(aes(x = sentiment, y = imdb_rating ))+
geom_point(aes(color = as.factor(season)))+
geom_smooth()+labs(x = "Sentiment level", y = "IMDb rating", title = "The relationship between the sentiment level and the IMDb rating of an episode", color = "Season")+scale_y_log10()
ratings_raw%>%mutate(total_ep = row_number())%>%
left_join(episode_sent)%>%
ggplot(aes(x = sentiment, y = imdb_rating ))+
geom_point(aes(color = as.factor(season)))+
geom_smooth()+labs(x = "Sentiment level", y = "IMDb rating", title = "The relationship between the sentiment level and the IMDb rating of an episode", color = "Season")
ratings_raw%>%mutate(total_ep = row_number())%>%
left_join(episode_sent)%>%
ggplot(aes(x = sentiment, y = imdb_rating,color = as.factor(season) ))+
geom_point(show.legend = FALSE)+geom_smooth()+facet_wrap(~season)+labs(x = "Sentiment level", y = "IMDb rating", title = "The relationship between the sentiment level and the IMDb rating of an episode")+theme(legend.position = "null")
library(tidytext)
office_script%>%count(season = as.factor(season), character)%>%
group_by(season)%>%top_n(10)%>%
ggplot(aes(x = n, y = reorder_within(character, n, season), fill = character))+
geom_col()+scale_y_reordered()+facet_wrap(~season,scales = "free_y")+theme(legend.position = "null")
office_script%>%count(season = as.factor(season), character)%>%
group_by(season)%>%top_n(10)%>%
ggplot(aes(x = n, y = reorder_within(character, n, season), fill = character))+
geom_col()+scale_y_reordered()+facet_wrap(~season,scales = "free_y")+theme(legend.position = "null")+labs(x = "Number of lines", y = "Character name", title = "Top 10 people with the most lines in each season")
top_character<-office_script%>%count(character,sort = TRUE)%>%top_n(12)%>%pull(character)
ggplotly(office_script%>%count(season, character)%>%
filter(character%in%top_character)%>%
ggplot(aes(x = season, y = n, color = character))+geom_point()+
geom_path()+scale_y_log10()+scale_x_continuous(breaks = 1:9))
ggplotly(office_script%>%count(season, character)%>%
filter(character%in%top_character)%>%
ggplot(aes(x = season, y = n, color = character))+geom_point()+
geom_path()+scale_y_log10()+scale_x_continuous(breaks = 1:9)+labs(x = "Season", y = "Number of lines", title = "Changing number of lines for each character"))
office_script%>%
select(season,episode, episode_name,character,text)%>%
unnest_tokens(word,text)%>%anti_join(stop_words)%>%
count(character,word)%>%filter(character%in%top_character)%>%
bind_tf_idf(word,character,n)%>%
group_by(character)%>%top_n(10)%>%ggplot(aes(x = tf_idf, y = reorder_within(word, tf_idf, character), fill = character))+geom_col(show.legend = FALSE)+facet_wrap(~character, scales = "free_y")+scale_y_reordered()
office_script%>%
select(season,episode, episode_name,character,text)%>%
unnest_tokens(word,text)%>%anti_join(stop_words)%>%
count(character,word)%>%filter(character%in%top_character)%>%
bind_tf_idf(word,character,n)%>%
group_by(character)%>%top_n(10)%>%ggplot(aes(x = tf_idf, y = reorder_within(word, tf_idf, character), fill = character))+geom_col(show.legend = FALSE)+facet_wrap(~character, scales = "free_y")+scale_y_reordered()+labs(y = "Words", title = "Unique words spoken by each characters")
character_season_line<-office_script%>%count(season, character)%>%
filter(character%in%top_character)
ggplotly(office_script%>%
select(season,episode, episode_name,character,text)%>%
unnest_tokens(word,text)%>%anti_join(stop_words)%>%
inner_join(get_sentiments("afinn"))%>%group_by(season,character)%>%
summarise(sentiment = sum(value))%>%filter(character%in%top_character)%>%
left_join(character_season_line)%>%mutate(av_sent = sentiment/n)%>%
ggplot(aes(x = season, y = av_sent, color = character))+geom_point()+geom_path())
ggplotly(office_script%>%
select(season,episode, episode_name,character,text)%>%
unnest_tokens(word,text)%>%anti_join(stop_words)%>%
inner_join(get_sentiments("afinn"))%>%group_by(season,character)%>%
summarise(sentiment = sum(value))%>%filter(character%in%top_character)%>%
left_join(character_season_line)%>%mutate(av_sent = sentiment/n)%>%
ggplot(aes(x = season, y = av_sent, color = character))+geom_point()+geom_path()+labs(x = "Season", y = "Average sentiment level", title = "Average sentiment level of each characters"))
theme_set(theme_classic())
ratings_raw%>%mutate(total_ep = row_number())%>%
ggplot(aes(x = total_ep, y = imdb_rating))+
geom_point(aes(color = as.factor(season)))+
geom_path(aes(color = as.factor(season)))+
geom_text(aes(label= title),hjust = 1,check_overlap = TRUE)+
theme(legend.position = "null")+expand_limits(x = -20)+
labs(x = "Episode number", y = "IMDb rating", title = "How the rating of each episode changes over time")+geom_smooth()
ratings_raw%>%
ggplot(aes(y = as.factor(season),
x = imdb_rating,
fill = as.factor(season),
group = as.factor(season)))+
geom_density_ridges(alpha = 0.5, show.legend = FALSE)+
expand_limits(y = 10.7)+
labs(x = "IMDb rating", y = "Season number", title = "IMDb rating distribution for each season")
episode_sent<-office_script%>%
select(season,episode, episode_name,character,text)%>%
unnest_tokens(word,text)%>%anti_join(stop_words)%>%
inner_join(get_sentiments("afinn"))%>%group_by(season,episode, episode_name)%>%
summarise(sentiment = sum(value))%>%ungroup()%>%
transmute(total_ep = row_number(),episode_name, sentiment,season)
episode_sent%>%
ggplot(aes(x = total_ep, y = sentiment))+
geom_point(aes(color = as.factor(season)))+
geom_path(aes(color = as.factor(season)))+
geom_text(aes(label= episode_name),hjust = 1,check_overlap = TRUE)+
theme(legend.position = "null")+expand_limits(x = -20)+
labs(x = "Episode number", y = "Sentiment level", title = "How the sentiment level of each epsiode changes over time")+geom_smooth()
episode_sent%>%
ggplot(aes(y = as.factor(season),
x = sentiment,
fill = as.factor(season),
group = as.factor(season)))+
geom_density_ridges(alpha = 0.5, show.legend = FALSE)+
expand_limits(y = 10.7)+
labs(x = "Sentiment level", y = "Season number", title = "Sentiment level distribution for each season")
library(ggrepel)
ratings_raw%>%mutate(total_ep = row_number())%>%
left_join(episode_sent)%>%
ggplot(aes(x = sentiment, y = imdb_rating ))+
geom_point(aes(color = as.factor(season)))+
geom_smooth()+labs(x = "Sentiment level", y = "IMDb rating", title = "The relationship between the sentiment level and the IMDb rating of an episode", color = "Season")
library(tidytext)
office_script%>%count(season = as.factor(season), character)%>%
group_by(season)%>%top_n(10)%>%
ggplot(aes(x = n, y = reorder_within(character, n, season), fill = character))+
geom_col()+scale_y_reordered()+facet_wrap(~season,scales = "free_y")+theme(legend.position = "null")+labs(x = "Number of lines", y = "Character name", title = "Top 10 people with the most lines in each season")
top_character<-office_script%>%count(character,sort = TRUE)%>%top_n(12)%>%pull(character)
ggplotly(office_script%>%count(season, character)%>%
filter(character%in%top_character)%>%
ggplot(aes(x = season, y = n, color = character))+geom_point()+
geom_path()+scale_y_log10()+scale_x_continuous(breaks = 1:9)+labs(x = "Season", y = "Number of lines", title = "Changing number of lines for each character"))
office_script%>%
select(season,episode, episode_name,character,text)%>%
unnest_tokens(word,text)%>%anti_join(stop_words)%>%
count(character,word)%>%filter(character%in%top_character)%>%
bind_tf_idf(word,character,n)%>%
group_by(character)%>%top_n(10)%>%ggplot(aes(x = tf_idf, y = reorder_within(word, tf_idf, character), fill = character))+geom_col(show.legend = FALSE)+facet_wrap(~character, scales = "free_y")+scale_y_reordered()+labs(y = "Words", title = "Unique words spoken by each characters")
character_season_line<-office_script%>%count(season, character)%>%
filter(character%in%top_character)
ggplotly(office_script%>%
select(season,episode, episode_name,character,text)%>%
unnest_tokens(word,text)%>%anti_join(stop_words)%>%
inner_join(get_sentiments("afinn"))%>%group_by(season,character)%>%
summarise(sentiment = sum(value))%>%filter(character%in%top_character)%>%
left_join(character_season_line)%>%mutate(av_sent = sentiment/n)%>%
ggplot(aes(x = season, y = av_sent, color = character))+geom_point()+geom_path()+labs(x = "Season", y = "Average sentiment level", title = "Average sentiment level of each characters"))
actors<-office_info%>%count(episode_name, character)%>%
add_count(character)%>%
filter(nn >800)%>%
select(-nn)%>%
pivot_wider(names_from = character, values_from = n, values_fill = 0)
remove_regex <- "[:punct:]|[:digit:]|parts |part |the |and"
office_ratings <- ratings_raw %>%
transmute(
episode_name = str_to_lower(title),
episode_name = str_remove_all(episode_name, remove_regex),
episode_name = str_trim(episode_name),
imdb_rating
)
office_info <- schrute::theoffice %>%
mutate(
season = as.numeric(season),
episode = as.numeric(episode),
episode_name = str_to_lower(episode_name),
episode_name = str_remove_all(episode_name, remove_regex),
episode_name = str_trim(episode_name)
) %>%
select(season, episode, episode_name, director, writer, character)
actors<-office_info%>%count(episode_name, character)%>%
add_count(character)%>%
filter(nn >800)%>%
select(-nn)%>%
pivot_wider(names_from = character, values_from = n, values_fill = 0)
actors
creators <- office_info%>%distinct(episode_name,director,writer)%>%
separate_rows(writer,sep = ";")%>%
pivot_longer(director:writer,names_to = "job",values_to = "name")%>%
add_count(name)%>%filter(n>10)%>%mutate(count = 1)%>%
distinct(episode_name,name,count)%>%
pivot_wider(names_from = name,values_from = count, values_fill = 0)
creators
library(vip)
set.seed(2345)
lasso_model <- linear_reg(penalty = 0.1, mixture = 1)%>%
set_engine("glmnet")
staff_wf<-workflow()%>%
add_recipe(staff_recipe)%>%
add_model(lasso_model)
staff_recipe<-recipe(rating~., data = staff_train)%>%
update_role(episode_name, new_role = "ID")%>%
step_zv(all_predictors())%>%
step_corr(all_predictors())%>%
step_normalize(all_predictors())
set.seed(1234)
staff_split<-initial_split(staff,strata = season)
rating_clean <- ratings_raw%>%transmute(season, episode,episode_name = title, rating = imdb_rating)%>%
mutate(
episode_name = str_to_lower(episode_name),
episode_name = str_remove_all(episode_name, remove_regex),
episode_name = str_trim(episode_name))
staff<-rating_clean%>%
inner_join(creators)%>%
inner_join(actors)
staff
set.seed(1234)
staff_split<-initial_split(staff,strata = season)
staff_train <- training(staff_split)
staff_test <- testing(staff_split)
set.seed(123)
staff_boot<-bootstraps(staff_train, strata = season)
staff_recipe<-recipe(rating~., data = staff_train)%>%
update_role(episode_name, new_role = "ID")%>%
step_zv(all_predictors())%>%
step_corr(all_predictors())%>%
step_normalize(all_predictors())
prep(staff_recipe)%>%juice()
library(vip)
set.seed(2345)
lasso_model <- linear_reg(penalty = 0.1, mixture = 1)%>%
set_engine("glmnet")
staff_wf<-workflow()%>%
add_recipe(staff_recipe)%>%
add_model(lasso_model)
staff_wf%>%fit(staff_train)%>%pull_workflow_fit()%>%
tidy()
staff_wf%>%fit(staff_train)%>%pull_workflow_fit()%>%
tidy()%>%arrange(desc(term))
staff_wf%>%fit(staff_train)%>%pull_workflow_fit()%>%
tidy()%>%arrange(desc(estimate))
tune_model <- linear_reg(penalty = tune())%>%set_engine("glmnet")
lambda_grid <- grid_regular(penalty(), levels = 50)
tune_grid<-tune_grid(workflow()%>%add_recipe(staff_recipe)%>%add_model(tune_model),
resamples = staff_boot,
grid = lambda_grid)
tune_grid%>%collect_metrics()
tune_grid%>%collect_metrics()%>%
ggplot(aes(x = penalty, y = mean, color = .metric))+
geom_point()+
geom_line()+facet_wrap(~.metric,scales = "free",ncol = 1)+scale_x_log10()
final_lasso%>%fit(staff_train)%>%pull_workflow_fit()%>%vi(lamda = best_penalty$penalty)%>%
mutate(Importance = case_when(Sign == "POS"~Importance,
TRUE~(-1)*Importance))%>%
ggplot(aes(x = Importance, y = reorder(Variable, Importance), color = Sign))+
geom_point(size = 2)+
geom_segment(aes(x = 0, xend = Importance, y = Variable, yend = Variable),size = 1.5)
best_penalty<-tune_grid%>%select_best("rmse",maximize = FALSE)
best_penalty
final_lasso<-finalize_workflow(workflow()%>%add_recipe(staff_recipe)%>%add_model(tune_model),
best_penalty)
final_lasso
final_lasso%>%fit(staff_train)%>%pull_workflow_fit()%>%vi(lamda = best_penalty$penalty)%>%
mutate(Importance = case_when(Sign == "POS"~Importance,
TRUE~(-1)*Importance))%>%
ggplot(aes(x = Importance, y = reorder(Variable, Importance), color = Sign))+
geom_point(size = 2)+
geom_segment(aes(x = 0, xend = Importance, y = Variable, yend = Variable),size = 1.5)
final_lasso%>%last_fit(staff_split)%>%collect_metrics()
blogdown:::serve_site()
blogdown:::serve_site()
class(full_news$date)
library(readr)
library(tidyverse)
library(tidytext)
library(lubridate)
library(DT)
library(plotly)
Fake <- read_csv("Fake.csv")
True <- read_csv("True.csv")
fake<-Fake%>%mutate(type = "fake")
true<-True%>%mutate(type = "true")
full_news<-fake%>%rbind(true)
full_news<-full_news%>%mutate(num=1:dim(full_news)[1])%>%select(num,everything())
ggplotly(full_news%>%count(type, sort = TRUE)%>%ggplot(aes(x = type, y = n, fill = type))+geom_col())
ggplotly(full_news%>%count(type, subject, sort = TRUE)%>%ggplot(aes(x = reorder(subject,n), y = n, fill = type))+geom_col()+coord_flip())
class(full_news$date)
time_news<-full_news%>%mutate(time = mdy(date))%>%select(-date)
time_news%>%head(10)
class(full_news$date)
time_news<-full_news%>%mutate(time = mdy(date))%>%select(-date)
time_news%>%head(6)
ggplotly(time_news%>%group_by(time,type)%>%summarise(total_time = sum(n()))%>%ggplot(aes(x = time, y = total_time, color=type))+geom_point(alpha = 0.5)+scale_y_log10()+labs(title = "Number of true news and fake news across time", x = "Time", y = "Count"))
ggplotly(time_news%>%group_by(time,type)%>%summarise(total_time = sum(n()))%>%ggplot(aes(x = time, y = total_time, color=type))+geom_point(alpha = 0.3)+scale_y_log10()+labs(title = "Number of true news and fake news across time", x = "Time", y = "Count")+geom_smooth())
token_title<-time_news%>%select(-text)%>%unnest_tokens(title_word,title)%>%filter(!title_word %in% stop_words$word)
token_text<-time_news%>%select(-title)%>%unnest_tokens(text_word,text)%>%filter(!text_word %in% stop_words$word)
ggplotly(token_title%>%count(type,title_word, sort = TRUE)%>%group_by(type)%>%top_n(20)%>%ggplot(aes(x = n, y = reorder(title_word,n), fill = type))+geom_col()+facet_wrap(~type, scales = "free")+scale_x_log10())
library(tidytext)
ggplotly(token_title%>%count(type,title_word, sort = TRUE)%>%group_by(type)%>%top_n(20)%>%ggplot(aes(x = n, y = reorder_within(title_word,n,type), fill = type))+geom_col()+facet_wrap(~type, scales = "free")+scale_x_log10()+scale_y_reordered())
ggplotly(token_text%>%count(type,text_word, sort = TRUE)%>%group_by(type)%>%top_n(20)%>%ggplot(aes(x = n, y = reorder(text_word,n), fill = type))+geom_col()+facet_wrap(~type, scales = "free"))
ggplotly(token_text%>%count(type,text_word, sort = TRUE)%>%group_by(type)%>%top_n(20)%>%ggplot(aes(x = n, y = reorder_within(text_word,n,type), fill = type))+geom_col()+facet_wrap(~type, scales = "free")+scale_y_reordered())
library(widyr)
library(igraph)
library(ggraph)
true_title_pair_count<-token_title%>%filter(type=="true")%>%pairwise_count(title_word,num)%>%arrange(desc(n))%>%filter(n>70)
true_title_pair_count%>%head(150)%>%datatable()
true_title_graph<-true_title_pair_count%>%graph_from_data_frame()
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggplotly(ggraph(true_title_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void())
fake_title_pair_count<-token_title%>%filter(type=="fake")%>%pairwise_count(title_word,num)%>%arrange(desc(n))%>%filter(n>80)
fake_title_pair_count%>%head(150)%>%datatable()
fake_title_graph<-fake_title_pair_count%>%graph_from_data_frame()
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggplotly(ggraph(fake_title_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "pink", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void())
ggplotly(token_title%>%inner_join(get_sentiments("afinn"), by = c(title_word="word"))%>%group_by(type,title_word)%>%summarise(occurance = sum(n()), contribution = sum(n()*value))%>%arrange(desc(occurance))%>%top_n(20,abs(contribution))%>%ggplot(aes(x = contribution, y = reorder(title_word,contribution), fill = contribution>0))+geom_col()+facet_wrap(~type,scales = "free_y"))
ggplotly(token_title%>%inner_join(get_sentiments("afinn"), by = c(title_word="word"))%>%group_by(type,title_word)%>%summarise(occurance = sum(n()), contribution = sum(n()*value))%>%arrange(desc(occurance))%>%top_n(20,abs(contribution))%>%ggplot(aes(x = contribution, y = reorder_within(title_word,contribution,type), fill = contribution>0))+geom_col()+facet_wrap(~type,scales = "free_y")+scale_y_reordered())
ggplotly(token_text%>%inner_join(get_sentiments("afinn"), by = c(text_word="word"))%>%group_by(type,text_word)%>%summarise(occurance = sum(n()), contribution = sum(n()*value))%>%arrange(desc(occurance))%>%top_n(20,abs(contribution))%>%ggplot(aes(x = contribution, y = reorder_within(text_word,contribution,type), fill = contribution>0))+geom_col()+facet_wrap(~type,scales = "free_y")+scale_y_reordered())
av_fake_title_num_sent<-token_title%>%filter(type=="fake")%>%count(num,title_word)%>%inner_join(get_sentiments("afinn"), by = c(title_word = "word"))%>%group_by(num)%>%summarise(sent = sum(n*value))
av_fake_title_num_sent%>%datatable()
mean(av_fake_title_num_sent$sent)
av_true_title_num_sent<-token_title%>%filter(type=="true")%>%count(num,title_word)%>%inner_join(get_sentiments("afinn"), by = c(title_word = "word"))%>%group_by(num)%>%summarise(sent = sum(n*value))
av_true_title_num_sent%>%datatable()
mean(av_true_title_num_sent$sent)
full_title<-av_fake_title_num_sent%>%mutate(type = "fake")%>%rbind(av_true_title_num_sent%>%mutate(type = "true"))
full_title%>%ggplot(aes(x=NULL, y = sent))+geom_boxplot()+facet_wrap(~type)
full_title%>%ggplot(aes(x=NULL, y = sent, fill= type))+geom_boxplot()+facet_wrap(~type)
title_num<-token_title%>%count(num)%>%transmute(total=n,num)
title_num%>%datatable()
title_num<-token_title%>%count(num)%>%transmute(total=n,num)
title_num<-token_title%>%count(num)%>%transmute(total=n,num)
title_num%>%datatable()
av_fake_title_num_sent_just<-token_title%>%filter(type=="fake")%>%count(num,title_word)%>%inner_join(get_sentiments("afinn"), by = c(title_word = "word"))%>%left_join(title_num,by = c(num="num"))%>%group_by(num)%>%summarise(av_sent = sum(n*value)/mean(total))
av_fake_title_num_sent_just%>%datatable()
fake_title_graph<-fake_title_pair_count%>%graph_from_data_frame()
set.seed(2020)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggplotly(ggraph(fake_title_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "pink", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void())
mean(av_fake_title_num_sent_just$av_sent)
av_true_title_num_sent_just<-token_title%>%filter(type=="true")%>%count(num,title_word)%>%inner_join(get_sentiments("afinn"), by = c(title_word = "word"))%>%left_join(title_num,by = c(num="num"))%>%group_by(num)%>%summarise(av_sent = sum(n*value)/mean(total))
av_true_title_num_sent_just%>%datatable()
mean(av_true_title_num_sent_just$av_sent)
av_fake_text_num_sent<-token_text%>%filter(type=="fake")%>%count(num,text_word)%>%inner_join(get_sentiments("afinn"), by = c(text_word = "word"))%>%group_by(num)%>%summarise(sent = sum(n*value))
av_fake_text_num_sent%>%head(4000)%>%datatable()
full_text<-av_fake_text_num_sent%>%mutate(type = "fake")%>%rbind(av_true_text_num_sent%>%mutate(type = "true"))
av_true_text_num_sent<-token_text%>%filter(type=="true")%>%count(num,text_word)%>%inner_join(get_sentiments("afinn"), by = c(text_word = "word"))%>%group_by(num)%>%summarise(sent = sum(n*value))
av_true_text_num_sent%>%head(4000)%>%datatable()
mean(av_true_text_num_sent$sent)
full_text<-av_fake_text_num_sent%>%mutate(type = "fake")%>%rbind(av_true_text_num_sent%>%mutate(type = "true"))
full_text%>%ggplot(aes(x=NULL, y = sent))+geom_boxplot()+facet_wrap(~type)+ylim(-200,100)
full_text%>%ggplot(aes(x=NULL, y = sent))+geom_boxplot()+facet_wrap(~type)+ylim(-200,100)+scale_y_log10()
text_num<-token_text%>%count(num)%>%transmute(total=n,num)
text_num%>%datatable()
av_fake_text_num_sent_just<-token_text%>%filter(type=="fake")%>%count(num,text_word)%>%inner_join(get_sentiments("afinn"), by = c(text_word = "word"))%>%left_join(text_num,by = c(num="num"))%>%group_by(num)%>%summarise(av_sent = sum(n*value)/mean(total))
av_fake_text_num_sent_just%>%head(4000)%>%datatable()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
campus%>%
ggplot(aes(x = high_school_score, fill = status))+
geom_histogram(aes(y = ..density..), alpha = 0.5, color = "white")+
geom_density(alpha = 0.6)+scale_fill_brewer(palette = "Paired")+
labs(x = "Scores at high school", y = "Density", title = "Distribution of scores when the applicants were in high school")
blogdown:::serve_site()
blogdown:::serve_site()
library(tidyverse)
library(tidymodels)
library(plotly)
theme_set(theme_light())
friends <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends.csv')
friends%>%count(speaker)%>%
filter(speaker!="NA")%>%
filter(n>120)%>%arrange(desc(n))%>%
ggplot(aes(x = reorder(speaker,n), y = n, fill = speaker))+
geom_col(show.legend = FALSE,alpha= 0.7)+coord_polar()+scale_y_log10()+
scale_color_brewer(palette = "Dark2")
library(tidyverse)
library(tidymodels)
library(plotly)
theme_set(theme_light())
friends <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends.csv')
friends_emotions <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends_emotions.csv')
friends_info <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-08/friends_info.csv')
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
install.packages('rsconnect')
rsconnect::setAccountInfo(name='xuxinzhang',
token='A3A618BA934C2F072E0DB04B68C4A7CF',
secret='<SECRET>')
rsconnect::setAccountInfo(name='xuxinzhang',
token='A3A618BA934C2F072E0DB04B68C4A7CF',
secret='<SECRET>')
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
```{r,warning=FALSE}
cocktails$measure
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
---
title: How does the US spending on kids change?
author: Xuxin Zhang
date: '2020-09-22'
slug: post
categories:
- R project
tags:
- data visualization
- gganimate
subtitle: ''
summary: ''
authors: []
lastmod: '2020-09-22T16:33:37+08:00'
featured: no
image:
caption: ''
focal_point: ''
preview_only: yes
projects: []
---
## 1. Introduction
Hifsdf
blogdown:::serve_site()
blogdown:::serve_site()
